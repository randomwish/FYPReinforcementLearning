Steps,Policy/Extrinsic Value Estimate,Policy/Entropy,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Environment/Episode Length,Environment/Cumulative Reward,Policy/Extrinsic Reward,Is Training
40000,0.21681243,1.423093,0.0063841846,0.02256883,0.00029688966,1999.0,1.6131940494394965,1.6131940494394965,1.0
50000,0.16397998,1.4228127,0.009328634,0.02386282,0.00029582405,None,None,None,1.0
60000,0.09144777,1.4232574,0.0037582533,0.024943246,0.0002947872,None,None,None,1.0
70000,0.07015871,1.4241728,0.0011570171,0.025598336,0.00029375037,None,None,None,1.0
80000,0.12885213,1.4248401,0.0028155644,0.020579046,0.00029268482,1999.0,-0.22165569621655676,-0.22165569621655676,1.0
90000,0.083819196,1.4249599,0.01182032,0.024315538,0.00029164803,None,None,None,1.0
100000,0.0876107,1.4252033,0.004716781,0.021117516,0.00029061118,None,None,None,1.0
110000,0.120148085,1.4261166,0.0039042258,0.020753762,0.00028957438,1999.0,1.0017552566197183,1.0017552566197183,1.0
120000,0.14541237,1.4270606,0.013041119,0.025419172,0.00028850883,None,None,None,1.0
130000,0.16402577,1.4277118,0.011619995,0.02809469,0.00028747198,None,None,None,1.0
140000,0.15756024,1.4282238,0.010760474,0.024865838,0.00028643518,None,None,None,1.0
