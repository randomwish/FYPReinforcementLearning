Steps,Policy/Extrinsic Value Estimate,Policy/Entropy,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Environment/Episode Length,Environment/Cumulative Reward,Policy/Extrinsic Reward,Is Training
20000,0.25241944,2.1915984,2.0216796,0.024126222,0.00029841607,999.0,-49.93333035310109,-49.93333035310109,1.0
30000,-0.29328135,2.1965928,1.1607221,0.024435986,0.00029674202,999.0,-49.92856844833919,-49.92856844833919,1.0
40000,-0.2916613,2.1913865,0.9497859,0.025672259,0.00029506805,None,-49.99999701976776,-49.99999701976776,1.0
50000,-0.62886643,2.187964,0.27678838,0.028769718,0.00029348402,999.0,-49.666663686434426,-49.666663686434426,1.0
60000,-1.0997256,2.1967854,1.1838796,0.018808607,0.00029181005,999.0,-49.785711305482046,-49.785711305482046,1.0
70000,-1.0326421,2.1844404,0.7762205,0.026669336,0.000290136,None,-49.99999701976776,-49.99999701976776,1.0
80000,-1.2714808,2.1889243,0.519369,0.030469595,0.00028846198,999.0,-49.93333035310109,-49.93333035310109,1.0
90000,-1.4963278,2.1960912,0.2820722,0.016368395,0.00028687803,999.0,-49.64285416262491,-49.64285416262491,1.0
100000,-1.6378886,2.1876435,0.4796749,0.024872504,0.00028520406,None,-48.99999701976776,-48.99999701976776,1.0
110000,-1.7153071,2.1875572,0.34669304,0.02393917,0.00028353,999.0,-49.733330353101096,-49.733330353101096,1.0
